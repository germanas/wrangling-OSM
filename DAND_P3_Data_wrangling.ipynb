{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrangling osm data for Vilnius, Lithuania\n",
    "## Map area\n",
    "\n",
    "Vilnius, Lithuania  \n",
    "https://www.openstreetmap.org/relation/1529146#map=11/54.7010/25.2528&layers=HN \n",
    "\n",
    "The file:  \n",
    "vilniusmap.osm - 177,989 Kb\n",
    "\n",
    "For this project I chose map data from Vilnius, the capital city of Lithuania. I am currently living here so this is the data set I wanted to explore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important code files for this project:\n",
    "\n",
    "* data.py - The main shaping file which takes my osm data, fixes it, and shapes it to CSV.\n",
    "* importtosql.py - The file that contains the code to import contents from CSV file to database.\n",
    "* TagTypes.py - Auditing file to check the types of tags, also checks for problems in those tags.\n",
    "* audit.py - Auditing file to check all of the bad street types that I will have to fix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems in the Map\n",
    "After auditing my map file with TagTypes.py and audit.py, I found these problems:  \n",
    "* Checking my map file with TagTypes showed me that I had 222212 lowercase tags, 242214 tags that were separated by colon (addr:street), 685 other tags that consisted of numbers from 1 to 685 and 0 problematic characters. I decided that I will clean my data to split the tags with semicolon, so that the characters before the colon will be set as the tag type and the characters after the colon will be ser as tag key.\n",
    "\n",
    "<img src=\"http://i.imgur.com/MkZwQBR.png\"> \n",
    "\n",
    "* After printing some of the tags, I found that some of the street values consists of shortened street types. For example g. should mean gatvė in Lithuanian. I created a python script to audit it and check for the street types to let me know what street types I will need to change later in the data.py file. I will explain how I fixed it later in the document. \n",
    "\n",
    "<img src=\"http://i.imgur.com/5IdBF8h.png\">\n",
    "\n",
    "* I could not import my data to sqlite3 database manualy. All I got was this error `INSERT failed: UNIQUE constraint failed.` That could have been because of special lithuanian characters like ė, ą, ų etc. in my data. I found out that on of the solutions was to create a python script to import the data to the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing colons in the tags\n",
    "I had to split the addr:street key so that the \"addr\" would go to the type field and \"street\" to key field. To fix the colons I had to create a function that takes each element of the node and way tag, iterates it, find the key values matching the regex for a string with colon and splitting the string to 2 values: key and type.\n",
    "\n",
    "```python\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS,\n",
    "                  problem_chars=PROBLEMCHARS, default_tag_type='regular'):\n",
    "\n",
    "    node_attribs = {}\n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  # Handle secondary tags the same way for both node and way elements\n",
    "    if element.tag == 'node':\n",
    "        for attrib in element.attrib:\n",
    "            if attrib in NODE_FIELDS:\n",
    "                node_attribs[attrib] = element.attrib[attrib]\n",
    "        for child in element:\n",
    "            node_tag = {}\n",
    "            if LOWER_COLON.match(child.attrib['k']): #Checks if the child attribute \"key\" matches the regex.\n",
    "                node_tag['type'] = child.attrib['k'].split(':',1)[0]\n",
    "```\n",
    "\n",
    "After running this code my key attributes were clean and without colons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixing Lithuanian street types\n",
    "After auditing my data for the street types, I found out that mostly all of my street values consists of shortened street types. For example I had street types like g. for gatvė, al. for alėja, skg. for skersgatvis. To fix this I came up with an idea to include a new helper function to my data shaping file data.py. The helper function checked if the values of ``` node_tag['value'],  way_tag['value'] ``` matches regex values for all of the street types, if it does, then it replaces the wrong street type with the right one. The helper function fix_streets is called in shape_element function.\n",
    "\n",
    "```python\n",
    "for child in element:\n",
    "            way_tag = {}\n",
    "            way_node = {}\n",
    "            if child.tag == 'tag':\n",
    "                if LOWER_COLON.match(child.attrib['k']):\n",
    "                    way_tag['type'] = child.attrib['k'].split(':',1)[0]\n",
    "                    way_tag['key'] = child.attrib['k'].split(':',1)[1]\n",
    "                    way_tag['id'] = element.attrib['id']\n",
    "                    way_tag['value'] = fix_streets(child.attrib['v'])\n",
    "                    print way_tag['value'] #Checking if it works\n",
    "                    tags.append(way_tag)\n",
    "                elif PROBLEMCHARS.match(child.attrib['k']):\n",
    "                    continue\n",
    "                else:\n",
    "                    way_tag['type'] = 'regular'\n",
    "                    way_tag['key'] = child.attrib['k']\n",
    "                    way_tag['id'] = element.attrib['id']\n",
    "                    way_tag['value'] = fix_streets(child.attrib['v'])\n",
    "                    tags.append(way_tag)\n",
    "            elif child.tag == 'nd':\n",
    "                way_node['id'] = element.attrib['id']\n",
    "                way_node['node_id'] = child.attrib['ref']\n",
    "                way_node['position'] = position\n",
    "                position += 1\n",
    "                way_nodes.append(way_node)\n",
    "        return {'way': way_attribs, 'way_nodes': way_nodes, 'way_tags': tags}\n",
    "    print tags\n",
    " \n",
    " \n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "#This function takes an attribute of a node, of way, then checks it against RegEx and if it matches, it will replace the words.\n",
    "def fix_streets(attribute):\n",
    "    if FIND_GATVE.match(attribute):\n",
    "        return attribute.replace('g.', 'gatvė'.decode(\"utf-8\"))\n",
    "    elif FIND_ALEJA.match(attribute):\n",
    "        return attribute.replace('al.', 'alėja'.decode(\"utf-8\"))\n",
    "    elif FIND_AIKSTE.match(attribute):\n",
    "        return attribute.replace('a.', 'aikštė'.decode(\"utf-8\"))\n",
    "    elif FIND_AKLIGATVIS.match(attribute):\n",
    "        return attribute.replace('aklg.', 'akligatvis')\n",
    "    elif FIND_KELIAS.match(attribute):\n",
    "        return attribute.replace('kel.', 'kelias')\n",
    "    elif FIND_PLENTAS.match(attribute):\n",
    "        return attribute.replace('pl.', 'plentas')\n",
    "    elif FIND_PROSPEKTAS.match(attribute):\n",
    "        return attribute.replace('pr.', 'prospektas')\n",
    "    elif FIND_SKERSGATVIS.match(attribute):\n",
    "        return attribute.replace('skg.', 'skersgatvis')\n",
    "    else:\n",
    "        return attribute\n",
    "```\n",
    "Fixing this resulted in cleaner data for my street types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing to database.\n",
    "For some time I was not able to import my csv files to sqlite. I tried importing the schema, created tables manualy, but nothing worked. I found out that 2 of these solutions would work:\n",
    "* Removing header lines from CSV files\n",
    "* Write a script to import the CSV's\n",
    "After reading this discussion on udacity forums https://discussions.udacity.com/t/insert-failed-unique-constraint-failed-nodes-id/199847/2 I realised that It might be something to do with my non ASCII characters and that I should try to write a script to add the values.  \n",
    "After the script was done, I was still getting 'ACSII' errors, and find out that decoding the data using .decode(\"utf-8\") helped. This is a snippet of code from my importtosql.py file:\n",
    "```python\n",
    "   # Read in the csv file as a dictionary, format the\n",
    "    # data as a list of tuples:\n",
    "    with open('nodes.csv','rb') as fin:\n",
    "        dr = csv.DictReader(fin) # comma is default delimiter\n",
    "        to_db = [(i['id'], i['lat'],i['lon'], i['user'].decode(\"utf-8\"), i['uid'], i['changeset'], i['timestamp']) for i in dr]\n",
    "\n",
    "    with open('nodes_tags.csv','rb') as fin2:\n",
    "        dr2 = csv.DictReader(fin2) # comma is default delimiter\n",
    "        to_db2 = [(i['id'], i['key'].decode(\"utf-8\"),i['value'].decode(\"utf-8\"), i['type']) for i in dr2]\n",
    "        pprint(to_db2)\n",
    "\n",
    "    with open('ways.csv','rb') as fin3:\n",
    "        dr3 = csv.DictReader(fin3) # comma is default delimiter\n",
    "        to_db3 = [(i['id'], i['user'].decode(\"utf-8\"),i['uid'].decode(\"utf-8\"), i['version'], i['changeset'], i['timestamp']) for i in dr3]\n",
    "\n",
    "    with open('ways_tags.csv','rb') as fin4:\n",
    "        dr4 = csv.DictReader(fin4) # comma is default delimiter\n",
    "        to_db4 = [(i['id'], i['key'].decode(\"utf-8\"),i['value'].decode(\"utf-8\"), i['type']) for i in dr4]\n",
    "\n",
    "    with open('ways_nodes.csv','rb') as fin5:\n",
    "        dr5 = csv.DictReader(fin5) # comma is default delimiter\n",
    "        to_db5 = [(i['id'], i['node_id'],i['position']) for i in dr5]\n",
    "\n",
    "    # insert the formatted data\n",
    "    cur.executemany(\"INSERT INTO nodes(id, lat, lon, user, uid, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?, ?);\", to_db)\n",
    "    cur.executemany(\"INSERT INTO nodes_tags(id, key, value, type) VALUES (?, ?, ?, ?);\", to_db2)\n",
    "    cur.executemany(\"INSERT INTO ways(id, user, uid, version, changeset, timestamp) VALUES (?, ?, ?, ?, ?, ?);\", to_db3)\n",
    "    cur.executemany(\"INSERT INTO ways_tags(id, key, value, type) VALUES (?, ?, ?, ?);\", to_db4)\n",
    "    cur.executemany(\"INSERT INTO ways_nodes(id, node_id, position) VALUES (?, ?, ?);\", to_db5)\n",
    "    # commit the changes\n",
    "    conn.commit()\n",
    "```\n",
    "After my data was cleaned and imported to SQL I could start querying the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the data\n",
    "This section provides basic statistics about the dataset. For a database I am using SQLite.\n",
    "### File sizes\n",
    "```\n",
    "03/02/2017  09:10 PM    <DIR>          .\n",
    "03/02/2017  09:10 PM    <DIR>          ..\n",
    "03/02/2017  09:10 PM    <DIR>          .ipynb_checkpoints\n",
    "03/02/2017  08:33 PM             1,546b audit.py\n",
    "03/02/2017  09:10 PM            11,425b DAND_P3_Data_wrangling.ipynb\n",
    "03/02/2017  08:38 PM             8,841b data.py\n",
    "03/02/2017  06:40 PM               866b getusers.py\n",
    "03/02/2017  06:40 PM             3,835b importtosql.py\n",
    "03/02/2017  06:40 PM               992b iterativeparsing.py\n",
    "03/02/2017  06:40 PM                21b README.md\n",
    "03/02/2017  06:40 PM             2,578b schema.py\n",
    "03/02/2017  08:35 PM             1,174b TagTypes.py\n",
    "03/02/2017  06:40 PM               232b testregex.py\n",
    "02/25/2017  04:36 PM       182,260,000b vilniusmap.osm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique users\n",
    "\n",
    "Query to get number of nodes:\n",
    "```sql\n",
    "SELECT COUNT(DISTINCT(users.uid))\n",
    "FROM (SELECT uid FROM nodes UNION ALL SELECT uid FROM ways) users;\n",
    "```\n",
    "Results in 656"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Nodes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query to get number of nodes:\n",
    "```sql\n",
    "SELECT COUNT(*) FROM nodes;\n",
    "```\n",
    "Results in 791572"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of Ways\n",
    "```sql\n",
    "SELECT COUNT(*) FROM ways;\n",
    "```\n",
    "Results in 124591"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List of top 10 types of shops in this region\n",
    "\n",
    "```sql\n",
    "SELECT value, COUNT(*) FROM nodes_tags WHERE key = \"shop\" GROUP BY value ORDER BY COUNT(*) DESC LIMIT 10;\n",
    "```\n",
    "Results in \n",
    "``` \n",
    "hairdresser|104\n",
    "supermarket|82\n",
    "kiosk|75\n",
    "car_repair|73\n",
    "clothes|69\n",
    "convenience|63\n",
    "alcohol|40\n",
    "florist|35\n",
    "bakery|34\n",
    "books|30\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count of contributions by specific user \"jurkis\"\n",
    "```sql\n",
    "SELECT e.user, COUNT(*) FROM (SELECT user FROM nodes UNION ALL SELECT user FROM ways) e WHERE user=\"Jurkis\";\n",
    "```\n",
    "Results in Jurkis|57741"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top node tags added by this user\n",
    "```sql\n",
    "SELECT e.user,e.key, COUNT(*) FROM (nodes JOIN nodes_tags ON nodes.id = nodes_tags.id) e WHERE user = 'Jurkis' GROUP BY key ORDER BY COUNT(*) DESC LIMIT 20;\n",
    "```\n",
    "Results in \n",
    "```\n",
    "Jurkis|name|193\n",
    "Jurkis|amenity|132\n",
    "Jurkis|city|92\n",
    "Jurkis|housenumber|91\n",
    "Jurkis|street|91\n",
    "Jurkis|website|80\n",
    "Jurkis|opening_hours|76\n",
    "Jurkis|shop|76\n",
    "Jurkis|phone|68\n",
    "Jurkis|highway|66\n",
    "Jurkis|country|58\n",
    "Jurkis|natural|46\n",
    "Jurkis|crossing|45\n",
    "Jurkis|operator|36\n",
    "Jurkis|email|33\n",
    "Jurkis|level|28\n",
    "Jurkis|cuisine|27\n",
    "Jurkis|wheelchair|23\n",
    "Jurkis|postcode|22\n",
    "Jurkis|type|16\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional ideas\n",
    "### Fixing max speed values\n",
    "After querying the database to find  weird values, I found that a lot of maxspeed values are bad.\n",
    "```sql\n",
    "SELECT key, value, COUNT(*)\n",
    "FROM ways_tags WHERE key=\"maxspeed\" GROUP BY value ORDER BY COUNT(*) DESC;\n",
    "```\n",
    "Returns: \n",
    "```\n",
    "Key         Value           Count(*)\n",
    "\"maxspeed\"\t\"50\"\t        \"1865\"\n",
    "\"maxspeed\"\t\"LT:urban\"\t    \"1411\"\n",
    "\"maxspeed\"\t\"sign\"\t        \"780\"\n",
    "\"maxspeed\"\t\"40\"\t        \"356\"\n",
    "\"maxspeed\"\t\"60\"\t        \"335\"\n",
    "\"maxspeed\"\t\"20\"\t        \"285\"\n",
    "\"maxspeed\"\t\"70\"\t        \"194\"\n",
    "\"maxspeed\"\t\"90\"\t        \"134\"\n",
    "\"maxspeed\"\t\"80\"\t        \"101\"\n",
    "\"maxspeed\"\t\"30\"\t        \"62\"\n",
    "\"maxspeed\"\t\"LT:rural\"\t    \"37\"\n",
    "\"maxspeed\"\t\"10\"\t        \"35\"\n",
    "\"maxspeed\"\t\"100\"\t        \"33\"\n",
    "\"maxspeed\"\t\"5\"\t            \"9\"\n",
    "\"maxspeed\"\t\"LT:motorway\"\t\"9\"\n",
    "\"maxspeed\"\t\"130\"\t        \"6\"\n",
    "\"maxspeed\"\t\"120\"\t        \"2\"\n",
    "\"maxspeed\"\t\"LT:zone40\"\t    \"1\"\n",
    "```\n",
    "\n",
    "In my opinion these values are important for some users, and programs since it might calculate the travel times according to max speeds and this information should be as clean as possible. Fixing this would be kind of difficult because I don't know where to get the missing values from. I could statisticaly calculate the averages of max speed for that type of area/road etc and fill it with \"average value\". This solution would be bad for programs or people that need accurate data like GPS etc.. but might be usefull just for general purposes."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:DAND]",
   "language": "python",
   "name": "conda-env-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
